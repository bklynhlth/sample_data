{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032117cc-647d-492b-b5ac-45320a7b8462",
   "metadata": {},
   "source": [
    "# Interactive Demonstration\n",
    "## OpenWillis user tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdcf276-f32e-45c9-ae7b-153d22ed91c6",
   "metadata": {},
   "source": [
    "This notebook walks through an interactive demonstration of the basic functions in OpenWillis to process audio and video files. It is intended to help users get a sense of what it's like to work with OpenWillis in a jupyter notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2814e2-7ccd-4b34-a35c-b4f59b47b89c",
   "metadata": {},
   "source": [
    "__Note:__ Be sure that you have gone through the OpenWillis [installation steps](https://www.notion.so/brooklynhealth/Installing-OpenWillis-and-jupyter-notebook-14983a8fe047814b88ced7d3831791f2?pvs=12) prior to continuing. \n",
    "\n",
    "For the demo, users can install either `openwillis` or the separate subpackages `openwillis-voice`, `openwillis-transcribe`, `openwillis-speech`, and `openwillis-face`. \n",
    "\n",
    "Functions in `openwillis-gps` are not covered in this demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6e720-7d88-4060-b596-9e42faef3b91",
   "metadata": {},
   "source": [
    "First, we'll load the necessary libraries. Some warning messages may appear but these can be safely ignored if your environment is set up correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58582f91-8d66-4442-910a-adb0d980e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec696b40-175a-4da4-8885-15c9a2869d6c",
   "metadata": {},
   "source": [
    "Before getting into the analysis portion, we need to load some data we can work with. For this demonstration, we will use some sample audio and video files of a person reading from a list of [standardized sentences](https://www.cs.columbia.edu/~hgs/audio/harvard.html). \n",
    "\n",
    "We'll also look at two samples from mock clinical interviews as a use case for the speaker separation functions. \n",
    "\n",
    "This data can be loaded straight from GitHub onto your local machine and into this jupyter notebook environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d0228-ae16-4927-bf1b-7d8044f1d745",
   "metadata": {},
   "source": [
    "Next, weâ€™ll use git clone to import sample audio and video data from GitHub. First, change the directory path (below) to reference a local folder on your computer where you plan to store sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3da3e9-342e-4728-87cd-e337b6e2fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/michelleworthington/Documents/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a39e3-3b97-4c0a-96b1-338398ba21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/bklynhlth/sample_data.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4c804-3ad2-4a00-98c7-7a9ed9c95efc",
   "metadata": {},
   "source": [
    "Below, we'll organize these files so they are easy to access in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffbbe1-4a04-408f-a7d1-142476e251a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = 'sample_data/audio_files'\n",
    "interview_dir = 'sample_data/audio_files/multiple_speakers'\n",
    "video_dir = 'sample_data/video_files'\n",
    "baseline_dir = 'sample_data/video_files/baseline_videos'\n",
    "\n",
    "audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "interview_files = [f for f in os.listdir(interview_dir)]\n",
    "video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
    "bl_files = [f for f in os.listdir(baseline_dir)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93028c-a43e-453e-a7a7-18ed76e15c03",
   "metadata": {},
   "source": [
    "Let's check to make sure we have the correct files in each of `audio_files`, `interview_files`, `video_files`, and `bl_files`. We're only working with data from 5 videos/audio files, and 2 interview files, so we should expect 4 lists of files that correspond with 'sentences_audio.m4a', 'sentences_video.mp4', and 'sentences_bl_video.mp4', numbered 1-5 (plus interview_1clip.mp3 and interview_2clip.mp3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e94fc-923b-4d98-ab7d-291e5b74e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "[audio_files, interview_files, video_files, bl_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e1146b-b559-4b15-9b05-16237d513ad3",
   "metadata": {},
   "source": [
    "### __Workflow 1:__ Single speaker, vocal acoustics and speech characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e349e50-d7e1-4681-85b4-c5a3aa86ae09",
   "metadata": {},
   "source": [
    "This example maps onto the workflow described in the user tutorials [here](https://www.notion.so/brooklynhealth/Analyzing-audio-with-a-single-speaker-14983a8fe04781389f08dadbf0667381) to examine vocal acoustics and speech characterisitics for an audio file with a single speaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b22fe-c572-488b-809d-27c7674f5370",
   "metadata": {},
   "source": [
    "### 1.1 - Vocal acoustics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4adfb-25f8-4e54-8c4a-4cc13007aa2d",
   "metadata": {},
   "source": [
    "For this function, you should have installed either `openwillis` or `openwillis-voice`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ae6bc-ce88-40c4-9564-a7f79b371dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the # in front of the library you have installed and run this cell. \n",
    "\n",
    "# import openwillis as ow\n",
    "#import openwillis.voice as owv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14822fab-42ff-4178-b4b0-325faf75e845",
   "metadata": {},
   "source": [
    "#### 1.1a - Procssing a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc52123-772e-4432-935d-a471d3af4626",
   "metadata": {},
   "source": [
    "Now, we can proceed with our processing. First, we'll just process a single audio file from the 'audio_files' folder above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903c467-3198-434a-a72c-4e5a0aa9935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise, summary = ow.vocal_acoustics(audio_path = 'sample_data/audio_files/sentences_1_audio.wav', voiced_segments = False, option = 'simple')\n",
    "# change to owv.vocal_acoustics if working from openwillis-voice instead of openwillis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a709939-ce80-40f4-8db4-917a8bae6a45",
   "metadata": {},
   "source": [
    "Now let's take a look at our summary data to make sure it populated correctly. We can first specify that we want to print all rows and columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ff7b5-0af8-4705-9efe-83ce368d2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177c67d-cf11-47dd-adba-28cd7390721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6349f3-4984-4bef-af82-0d9995c59d56",
   "metadata": {},
   "source": [
    "If the columns populated, we should be in good shape! As a check, the 'f0_mean' value should be approximately 72. Notice that some of the final columns are 'NaN' - these features are specifically for the more advanced options and will not populate if 'simple' is specified in the 'option' parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363fcd83-09a3-4961-8571-f25386e2a710",
   "metadata": {},
   "source": [
    "#### 1.1b - Processing multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9371f578-8085-4250-ae29-26694c9455d3",
   "metadata": {},
   "source": [
    "Here, let's go ahead and run vocal acoustics on all 5 files in our folder using a for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a45af9-987d-4ab8-8bf1-89e8eff7e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'sample_data/audio_files'\n",
    "\n",
    "framewise_data = pd.DataFrame()\n",
    "summary_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "  if filename.endswith('.wav'):\n",
    "    audio_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Run vocal acoustics function\n",
    "    framewise, summary = ow.vocal_acoustics(audio_path = audio_path, voiced_segments = False, option = 'simple')\n",
    "    # change to owv.vocal_acoustics if working from openwillis-voice instead of openwillis\n",
    "\n",
    "    # Here, make sure we can identify each file by adding the name in the first column of the dataframe, remove '.wav' from the name\n",
    "    filename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Add filename column as the first column using insert()\n",
    "    framewise.insert(0, 'filename', filename_no_ext)\n",
    "    summary.insert(0, 'filename', filename_no_ext)\n",
    "\n",
    "    # Store results for each file in each dataframe\n",
    "    framewise_data = pd.concat([framewise_data, framewise], ignore_index=True)\n",
    "    summary_data = pd.concat([summary_data, summary], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b8053-60ef-4885-898d-9f22552fd631",
   "metadata": {},
   "source": [
    "Let's take a look at the first few rows of data to make sure it's looking good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ea9f7-5d7d-4b8e-8906-4ba5fe2e4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836883d-9c52-437d-a079-acb24ea87863",
   "metadata": {},
   "source": [
    "Below, we will save this output as a .csv so we can further analyze and run statistical tests on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9e463-0fd6-4f99-b240-d3d05d7ac551",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'sample_data' # can change to a different output path if desired\n",
    "output_filename = 'summary_data.csv'\n",
    "output_csv_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "summary_data.to_csv(output_csv_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fccc57-9ed7-4563-9949-c5c2ed4a82d9",
   "metadata": {},
   "source": [
    "### 1.2 - Speech characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c568419-cca9-463a-9a61-d44d438719e7",
   "metadata": {},
   "source": [
    "For this function, you should have installed either `openwillis` or `openwillis-transcribe` and `openwillis-speech`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdfdbe-c80f-415d-b0d3-0d2ba40b1778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the # in front of these libraries if you installed them individually and run this cell.\n",
    "# Do not run this if you have alredy imported the full openwillis library.\n",
    "\n",
    "# import openwillis.transcribe as owt\n",
    "# import openwillis.speech as ows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a993f-c72e-485b-a1f7-8d1fb6cea464",
   "metadata": {},
   "source": [
    "#### 1.2a - Processing a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf29a1d-d279-417c-b398-d0216bb757cf",
   "metadata": {},
   "source": [
    "Now we will continue with the speech characteristics function. First, we will need to transcribe our file, here we are using the 'vosk' transcription function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6792f67-c432-428c-8ff8-3c0f6a0693de",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_json, transcript_text = ow.speech_transcription_vosk(filepath = 'sample_data/audio_files/sentences_1_audio.wav')\n",
    "# change to owt.speech_transcription_vosk if working from openwillis-transcribe instead of openwillis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1635b-616e-4ca5-83f4-38dd94d3bc44",
   "metadata": {},
   "source": [
    "Then we will pass the JSON file from the transcription function directly to the speech characteristics function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd16110-1cc0-43f3-8eb6-d6b05cf314a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, turns, summary_sc = ow.speech_characteristics(json_conf = transcript_json, option = 'simple')\n",
    "# change to ows.speech_characteristics if working from openwillis-speech instead of openwillis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8980efb-0208-40b4-9eae-01ffc1c6c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine summary data \n",
    "\n",
    "summary_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d355255b-c2f6-45d3-9799-faeeed22aa53",
   "metadata": {},
   "source": [
    "As a check, 'file_length' should be 16.53 seconds. We'll see some NaNs here as well for some of the more advaned linguistic features that don't apply to this sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0418a-eb90-4ed4-a34d-ddbd573f0d3f",
   "metadata": {},
   "source": [
    "#### 1.2b - Processing multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb9d76-2c79-411a-ab87-4aaba263b2f5",
   "metadata": {},
   "source": [
    "The below code will run the speech characteristics function on multiple files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e2d72-48df-44b2-b9e6-416d8e492954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'sample_data/audio_files'\n",
    "\n",
    "word_data = pd.DataFrame()\n",
    "turns_data = pd.DataFrame()\n",
    "summary_sc_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "  if filename.endswith('.wav'):\n",
    "    audio_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Transcribe\n",
    "    transcript_json, transcript_text = ow.speech_transcription_vosk(filepath = audio_path)\n",
    "    # change to owt.speech_transcription_vosk if working from openwillis-transcribe instead of openwillis  \n",
    "    words, turns, summary_sc = ow.speech_characteristics(json_conf = transcript_json, option = 'simple')\n",
    "    # change to ows.speech_characteristics if working from openwillis-speech instead of openwillis\n",
    "\n",
    "    # Here, make sure we can identify each file by adding the name in the first column of the dataframe, remove '.wav' from the name\n",
    "    filename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Add filename column as the first column using insert()\n",
    "    words.insert(0, 'filename', filename_no_ext)\n",
    "    turns.insert(0, 'filename', filename_no_ext)\n",
    "    summary_sc.insert(0, 'filename', filename_no_ext)\n",
    "\n",
    "    # Store results for each file in each dataframe\n",
    "    word_data = pd.concat([word_data, words], ignore_index=True)\n",
    "    turns_data = pd.concat([turns_data, turns], ignore_index=True)\n",
    "    summary_sc_data = pd.concat([summary_sc_data, summary_sc], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829aa5a-ee21-4a12-8cb0-6ba34931e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e456e-8881-493c-b739-190a2a99af7a",
   "metadata": {},
   "source": [
    "### __Workflow 2:__ Multiple speakers, vocal acoustics and speech characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1df1c1-e846-4b0a-a36b-2f8fb9611122",
   "metadata": {},
   "source": [
    "For this workflow, you should have installed either `openwillis` or `openwillis-transcribe` and `openwillis-speech`. The main difference here as compared to Workflow 1 is that we will first need to separate the clinician and the participant so we are only analyzing the participant's speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee975708-886c-4d1f-88b3-12ff63913913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the # in front of these libraries if you installed them individually and run this cell.\n",
    "# Do not run this if you have alredy imported the full openwillis library.\n",
    "\n",
    "# import openwillis.transcribe as owt\n",
    "# import openwillis.speech as ows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadbd17e-bb5c-4ff8-9966-f45f12b6020f",
   "metadata": {},
   "source": [
    "### 2.1 - Separating speakers in the audio file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e127a-21d9-40b6-ab72-253098ef5fd6",
   "metadata": {},
   "source": [
    "#### 2.1a - Processing a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d056ad-a440-48b7-950d-9820cd52880b",
   "metadata": {},
   "source": [
    "First, we will transcribe the interview clip using the WhisperX model. The sample interviews are extracted from mock PANSS interviews, so we can specify this context in the code. This will help the model identify the 'participant' from the 'clinician' in the resulting separated audio files. \n",
    "\n",
    "__Note:__ the processing of this function is approximately proportionate to the file length. Here, we tried to keep the clips short (under 5 minutes) to move things along, but keep in mind this will take a few minutes, and keep this in mind for use in your own research. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab5324e-05a1-4091-ac21-4b58ed6ce21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcript_json, transcript_text = ow.speech_transcription_whisper(filepath = 'sample_data/audio_files/multiple_speakers/interview_2clip.wav', \n",
    "                                                                   model = 'tiny', \n",
    "                                                                   hf_token = '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', \n",
    "                                                                   compute_type = 'float32',\n",
    "                                                                   context = 'panss')\n",
    "# change to owt.speech_transcription_whisper if working from openwillis-transcribe instead of openwillis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b89d00-d9c8-44ff-97e7-da90ae3ab076",
   "metadata": {},
   "source": [
    "The next step is to separate the audio for each speaker based on the transcript above and save them as different files for downstream processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f38a5-7072-4256-876c-09ed6eebfde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'sample_data/audio_files/multiple_speakers/' # change to your local directory where the files are stroed\n",
    "\n",
    "speaker_dict = ow.speaker_separation_labels(filepath = 'sample_data/audio_files/multiple_speakers/interview_2clip.wav', transcript_json =  transcript_json)\n",
    "# change to owt.speaker_separation_labels if working from openwillis-transcribe instead of openwillis\n",
    "\n",
    "ow.to_audio(filepath = 'sample_data/audio_files/multiple_speakers/interview_2clip.wav', speaker_dict = speaker_dict, output_dir = output_dir)\n",
    "# change to owt.to_audio if working from openwillis-transcribe instead of openwillis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a142b92-4e4d-40e2-858f-c0d4c77e98e7",
   "metadata": {},
   "source": [
    "Now, if you go to your local folder, you will see two new files: one labeled 'interview_2clip_clinician.wav' and 'interview_2clip_participant.wav'. \n",
    "\n",
    "From here, we can return to Workflow 1 where we will re-transcribe _just_ the participant recording, and then run vocal acoustics and speech characteristics to examine the speech features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb536a3-cae5-4f5c-8732-c969dffa6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, turns, summary_sc = ow.speech_characteristics(json_conf = transcript_json, option = 'simple', speaker_label = 'participant')\n",
    "# change to ows.speech_characteristics if working from openwillis-speech instead of openwillis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72dfe9b-8e40-4038-bc11-c07d8ee93ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise, summary = ow.vocal_acoustics(audio_path = 'sample_data/audio_files/multiple_speakers/interview_2clip_participant.wav', voiced_segments = False, option = 'simple')\n",
    "# change to owv.vocal_acoustics if working from openwillis-voice instead of openwillis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c765a7-ae43-49a5-b545-f32a7e7abd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb19a0-285d-4a6b-8150-264daeb1e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f14df-e2d4-481f-8386-c2b44a497f99",
   "metadata": {},
   "source": [
    "### __Workflow 3:__ Single speaker, video processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05771d9f-181d-49f4-9214-3ab4ac3739ee",
   "metadata": {},
   "source": [
    "For this workflow, you should have installed either `openwillis` or `openwillis-face`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd40f86-7d3f-4eb1-aa03-5017655ec327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the # in front of this libraries if you installed them individually \n",
    "# Do not run this if you only installed the full openwillis library\n",
    "\n",
    "# import openwillis.face as owf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc82266-a9e4-4124-821c-9913da77ba7c",
   "metadata": {},
   "source": [
    "### 3.1 - Facial expressivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8a760-558c-452e-838d-e0e709e3e058",
   "metadata": {},
   "source": [
    "#### 3.1a - Processing a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e58aa-059b-441b-af68-0148d148caef",
   "metadata": {},
   "source": [
    "From here, let's take a look at the video data. We'll start with just running facial expressivity on a single video file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d8b5b-3961-475b-9689-ff24338940c5",
   "metadata": {},
   "source": [
    "The video used in this example is 18 seconds and estimated runtime of `facial_expressivity` is approximately the same. For longer videos, you should expect runtimes approximately proportionate to the video duration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee124e43-9574-48a7-894b-accdd081de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise_loc, framewise_disp, summary_fe = ow.facial_expressivity(filepath = 'sample_data/video_files/sentences_1_video.mp4', baseline_filepath = 'sample_data/video_files/baseline_videos/sentences_1_bl_video.mp4')\n",
    "# change to owf.facial_expressivity if working from openwillis-face instead of openwillis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342489b9-28da-4de9-9c6c-73fe42856dba",
   "metadata": {},
   "source": [
    "We can look at the output in a couple of ways. We can look at the `framewise_disp` output to get a sense of displacement for each facial landmark at each frame. This dataframe contains quite a bit of data, so we can also look at the `summary_fe` output which will give us an overall displacement summary for each composite facial area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c2c3a6-fbee-4bb6-ae5b-6c2b5d3c7897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "framewise_disp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62554ee9-e8bc-4c38-9b3f-0881e5d2f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9ca21-8d2d-4b7c-88f8-671e755efdf4",
   "metadata": {},
   "source": [
    "Just for demonstration, if we don't include a baseline video, the displacement calculations will differ: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a57689-4a8c-4864-931a-5567426e3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise_loc_nobl, framewise_disp_nobl, summary_fe_nobl = ow.facial_expressivity(filepath = 'sample_data/video_files/sentences_1_video.mp4')\n",
    "# change to owf.facial_expressivity if working from openwillis-face instead of openwillis\n",
    "\n",
    "summary_fe_nobl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d42239-02ad-41d2-bd37-6bd9b897c879",
   "metadata": {},
   "source": [
    "#### 3.1b - Processing multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af50809-2aab-4c52-b7f7-45fa78281437",
   "metadata": {},
   "source": [
    "When running this function on multiple video files, make sure to match the video file to the baseline file using a subject identifier as demonstrated in the for loop below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f53118-7ef2-4334-a046-bcc52085455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'sample_data/video_files'\n",
    "baseline_folder = 'sample_data/video_files/baseline_videos/'\n",
    "\n",
    "frames_data = pd.DataFrame()\n",
    "displacement_data = pd.DataFrame()\n",
    "summary_fe_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.mp4'):\n",
    "        video_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Extract identifier from filename (assuming a pattern like \"person1_video.mp4\")\n",
    "        identifier = \"_\".join(filename.split(\"_\")[:2])  \n",
    "        baseline_filename = f\"{identifier}_bl_video.mp4\"  # Construct baseline filename\n",
    "        baseline_filepath = os.path.join(baseline_folder, baseline_filename)\n",
    "        \n",
    "        # Run facial expressivity - this sample uses the same video as a baseline because the samples are of the same person\n",
    "        framewise_loc, framewise_disp, summary_fe = ow.facial_expressivity(filepath = video_path, baseline_filepath = baseline_filepath)\n",
    "        # change to owf.facial_expressivity if working from openwillis-face instead of openwillis\n",
    "    \n",
    "        # Here, make sure we can identify each file by adding the name in the first column of the dataframe, remove '.mp4' from the name\n",
    "        filename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Add filename column as the first column using insert()\n",
    "        framewise_loc.insert(0, 'filename', filename_no_ext)\n",
    "        framewise_disp.insert(0, 'filename', filename_no_ext)\n",
    "        summary_fe.insert(0, 'filename', filename_no_ext)\n",
    "\n",
    "        # Store results for each file in each dataframe\n",
    "        frames_data = pd.concat([frames_data, framewise_loc], ignore_index=True)\n",
    "        displacement_data = pd.concat([displacement_data, framewise_disp], ignore_index=True)\n",
    "        summary_fe_data = pd.concat([summary_fe_data, summary_fe], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe06a14-2b0b-4345-a080-f908309a66c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_fe_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9456b1b-1f95-4c41-8087-cca27f643587",
   "metadata": {},
   "source": [
    "### 3.2 - Emotional expressivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1f0b3-54a2-4968-a975-f5ac9014d25e",
   "metadata": {},
   "source": [
    "#### 3.2a - Processing a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d0d46-6d33-41d3-b45f-aad5a0670008",
   "metadata": {},
   "source": [
    "When running the `emotional_expressivity` function, be aware that the runtime is considerably slower than for `facial_expressivity`. For the 18 second video, processing time is approximately 50 seconds. For longer videos, plan for a processing time of about 2.5x the file length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea8330-11c0-444e-8313-018090efe79b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "framewise_ee, summary_ee = ow.emotional_expressivity(filepath = 'sample_data/video_files/sentences_1_video.mp4', baseline_filepath = 'sample_data/video_files/baseline_videos/sentences_1_bl_video.mp4')\n",
    "# change to owf.emotional_expressivity if working from openwillis-face instead of openwillis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afedb57-98a8-41a2-a947-47766272b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebba60a-f120-45e6-8e54-f4affa0e9f5f",
   "metadata": {},
   "source": [
    "Again for demonstration without a baseline video, the expressivity metrics will differ: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb359a3-dbdd-4a98-8590-b12ef124073b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "framewise_ee_nobl, summary_ee_nobl = ow.emotional_expressivity(filepath = 'sample_data/video_files/sentences_1_video.mp4')\n",
    "# change to owf.emotional_expressivity if working from openwillis-face instead of openwillis\n",
    "\n",
    "summary_ee_nobl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aade75-3f34-4402-bb63-916213cd7f8e",
   "metadata": {},
   "source": [
    "#### 3.2b - Processing multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0608f79-88eb-4456-b5f3-77438b6075d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'sample_data/video_files'\n",
    "baseline_folder = 'sample_data/video_files/baseline_videos/'  \n",
    "\n",
    "frames_ee_data = pd.DataFrame()\n",
    "summary_ee_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.mp4'):\n",
    "        video_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Extract identifier from filename (assuming a pattern like \"person1_video.mp4\")\n",
    "        identifier = \"_\".join(filename.split(\"_\")[:2])  \n",
    "        baseline_filename = f\"{identifier}_bl_video.mp4\"  # Construct baseline filename\n",
    "        baseline_filepath = os.path.join(baseline_folder, baseline_filename)\n",
    "\n",
    "        # Run emotional expressivity - this sample uses a clip from the same video as a baseline because the samples are of the same person\n",
    "        framewise_ee, summary_ee = ow.emotional_expressivity(filepath = video_path, baseline_filepath = baseline_filepath)\n",
    "        # change to owf.emotional_expressivity if working from openwillis-face instead of openwillis\n",
    "    \n",
    "        # Here, make sure we can identify each file by adding the name in the first column of the dataframe, remove '.mp4' from the name\n",
    "        filename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Add filename column as the first column using insert()\n",
    "        framewise_ee.insert(0, 'filename', filename_no_ext)\n",
    "        summary_ee.insert(0, 'filename', filename_no_ext)\n",
    "\n",
    "        # Store results for each file in each dataframe\n",
    "        frames_ee_data = pd.concat([frames_ee_data, framewise_ee], ignore_index=True)\n",
    "        summary_ee_data = pd.concat([summary_ee_data, summary_ee], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79458a56-e18f-4798-8399-fcd05bff7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ee_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc70e4-f734-43b0-8219-82ba91ac5a3f",
   "metadata": {},
   "source": [
    "### 3.3 - Eye blink rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a4ba15-c290-4eb2-91bf-d9c508076c10",
   "metadata": {},
   "source": [
    "#### 3.3a - Processing a single file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd95448-e24b-4d78-8d96-6f3777d090a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ear, blinks, summary = ow.eye_blink_rate(video = 'sample_data/video_files/sentences_1_video.mp4')\n",
    "# change to owf.eye_blink_rate if working from openwillis-face instead of openwillis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580e5e5-3677-427c-a1c2-b4906bf350dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eec867-3c85-4e18-aac6-6cbaf6856c2e",
   "metadata": {},
   "source": [
    "The above output tells us that there were 8 blinks in the recording and they occured at a rate of 27 blinks per minute (in this case, because the recording is only 18 seconds long, it is extrapolated to a full minute)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c4023-8f04-41ef-a83a-8688bc1fcfd9",
   "metadata": {},
   "source": [
    "#### 3.3b - Processing multiple files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed31e8-1206-47b5-9c0a-44081a456eff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_folder = 'sample_data/video_files' \n",
    "\n",
    "ear_data = pd.DataFrame()\n",
    "blinks_data = pd.DataFrame()\n",
    "summary_eb_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(video_folder):\n",
    "    if filename.endswith(('.mp4')):\n",
    "        video_path = os.path.join(video_folder, filename)\n",
    "\n",
    "        ear, blinks, summary_eb = ow.eye_blink_rate(video=video_path)\n",
    "        # change to owf.eye_blink_rate if working from openwillis-face instead of openwillis\n",
    "\n",
    "        # Remove file extension from filename\n",
    "        filename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Add filename column as the first column using insert()\n",
    "        ear.insert(0, 'filename', filename_no_ext)\n",
    "        blinks.insert(0, 'filename', filename_no_ext)\n",
    "        summary_eb.insert(0, 'filename', filename_no_ext)\n",
    "\n",
    "        # Store results for each file in each dataframe\n",
    "        ear_data = pd.concat([ear_data, ear], ignore_index=True)\n",
    "        blinks_data = pd.concat([blinks_data, blinks], ignore_index=True)\n",
    "        summary_eb_data = pd.concat([summary_eb_data, summary_eb], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d8a9c-cca2-4440-b0bd-bc44b912859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_eb_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
