{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032117cc-647d-492b-b5ac-45320a7b8462",
   "metadata": {},
   "source": [
    "# Interactive Demonstration\n",
    "## OpenWillis user tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdcf276-f32e-45c9-ae7b-153d22ed91c6",
   "metadata": {},
   "source": [
    "This notebook walks through an interactive demonstration of the basic functions in OpenWillis to process audio and video files. This demo currently addresses use cases with a single speaker. It is intended to help users get a sense of what it's like to work with OpenWillis in a jupyter notebook environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2814e2-7ccd-4b34-a35c-b4f59b47b89c",
   "metadata": {},
   "source": [
    "__Note:__ Be sure that you have gone through the OpenWillis [installation steps](https://www.notion.so/brooklynhealth/Installing-OpenWillis-and-jupyter-notebook-14983a8fe047814b88ced7d3831791f2?pvs=12) prior to continuing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6e720-7d88-4060-b596-9e42faef3b91",
   "metadata": {},
   "source": [
    "First, we'll load the necessary libraries. Some warning messages may appear but these can be safely ignored if your environment is set up correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58582f91-8d66-4442-910a-adb0d980e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openwillis as ow\n",
    "import tensorflow as tf\n",
    "import whisperx\n",
    "import moviepy.editor as mp\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec696b40-175a-4da4-8885-15c9a2869d6c",
   "metadata": {},
   "source": [
    "Before getting into the analysis portion, we need to load some data we can work with. For this demonstration, we will use some sample audio and video files of a person reading from a list of [standardized sentences](https://www.cs.columbia.edu/~hgs/audio/harvard.html).\n",
    "\n",
    "This data can be loaded straight from GitHub into this jupyter notebook environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a39e3-3b97-4c0a-96b1-338398ba21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/bklynhlth/sample_data.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4c804-3ad2-4a00-98c7-7a9ed9c95efc",
   "metadata": {},
   "source": [
    "Below, we'll organize these files so they are easy to access in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffbbe1-4a04-408f-a7d1-142476e251a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = 'sample_data/audio_files'\n",
    "video_dir = 'sample_data/video_files'\n",
    "baseline_dir = 'sample_data/video_files/baseline_videos'\n",
    "\n",
    "audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.m4a')]\n",
    "video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
    "bl_files = [f for f in os.listdir(baseline_dir)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93028c-a43e-453e-a7a7-18ed76e15c03",
   "metadata": {},
   "source": [
    "Let's check to make sure we have the correct files in each of `audio_files`, `video_files`, and `bl_files`. We're only working with data from 5 videos/audio files, so we should expect 3 lists of files that correspond with 'sentences_audio.m4a', 'sentences_video.mp4', and 'sentences_bl_video.mp4', numbered 1-5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e94fc-923b-4d98-ab7d-291e5b74e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "[audio_files, video_files, bl_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b22fe-c572-488b-809d-27c7674f5370",
   "metadata": {},
   "source": [
    "### 1 - Vocal acoustics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14822fab-42ff-4178-b4b0-325faf75e845",
   "metadata": {},
   "source": [
    "#### 1.1 - Single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa9bd3-a496-4876-93c7-7944d040c94d",
   "metadata": {},
   "source": [
    "Before we continue, you might notice that the audio files in these folders are .m4a. That means we need to first convert to .mp3 or .wav in order to run through OpenWillis. Let's do that first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d079b0-9d29-4d9f-881e-f7d8c914b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'sample_data/audio_files'\n",
    "output_folder = 'sample_data/audio_files'\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".m4a\"):\n",
    "      m4a_file = os.path.join(input_folder, filename)\n",
    "      wav_file = os.path.join(output_folder, filename[:-4] + \".wav\") \n",
    "\n",
    "      audio = mp.AudioFileClip(m4a_file)\n",
    "      audio.write_audiofile(wav_file)\n",
    "      audio.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc52123-772e-4432-935d-a471d3af4626",
   "metadata": {},
   "source": [
    "Now, we can proceed with our processing. First, we'll just process a single audio file from the 'audio_files' folder above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903c467-3198-434a-a72c-4e5a0aa9935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise, summary = ow.vocal_acoustics(audio_path = 'sample_data/audio_files/sentences_1_audio.wav', voiced_segments = False, option = 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a709939-ce80-40f4-8db4-917a8bae6a45",
   "metadata": {},
   "source": [
    "Now let's take a look at our summary data to make sure it populated correctly. We can first specify that we want to print all rows and columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ff7b5-0af8-4705-9efe-83ce368d2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177c67d-cf11-47dd-adba-28cd7390721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6349f3-4984-4bef-af82-0d9995c59d56",
   "metadata": {},
   "source": [
    "Looks good! Notice that some of the final columns are 'NaN' - these features are specifically for the more advanced options and will not populate if 'simple' is specified in the 'option' parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363fcd83-09a3-4961-8571-f25386e2a710",
   "metadata": {},
   "source": [
    "#### 1.2 - Multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9371f578-8085-4250-ae29-26694c9455d3",
   "metadata": {},
   "source": [
    "Here, let's go ahead and run vocal acoustics on all 5 files in our folder using a for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a45af9-987d-4ab8-8bf1-89e8eff7e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'sample_data/audio_files'\n",
    "\n",
    "framewise_data = pd.DataFrame()\n",
    "summary_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "  if filename.endswith('.wav'):\n",
    "    audio_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Run vocal acoustics function\n",
    "    framewise, summary = ow.vocal_acoustics(audio_path = audio_path, voiced_segments = False, option = 'simple')\n",
    "\n",
    "    # Here, make sure we can identify each file by adding the name in the first column of the dataframe, remove '.wav' from the name\n",
    "    filename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Add filename column as the first column using insert()\n",
    "    framewise.insert(0, 'filename', filename_no_ext)\n",
    "    summary.insert(0, 'filename', filename_no_ext)\n",
    "\n",
    "    # Store results for each file in each dataframe\n",
    "    framewise_data = pd.concat([framewise_data, framewise], ignore_index=True)\n",
    "    summary_data = pd.concat([summary_data, summary], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b8053-60ef-4885-898d-9f22552fd631",
   "metadata": {},
   "source": [
    "We'll examine the data to make sure it's looking okay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ea9f7-5d7d-4b8e-8906-4ba5fe2e4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836883d-9c52-437d-a079-acb24ea87863",
   "metadata": {},
   "source": [
    "Below, we will save this output as a .csv so we can further analyze and run statistical tests on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9e463-0fd6-4f99-b240-d3d05d7ac551",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'sample_data' # change to your output path\n",
    "output_filename = 'summary_data.csv'\n",
    "output_csv_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "summary_data.to_csv(output_csv_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fccc57-9ed7-4563-9949-c5c2ed4a82d9",
   "metadata": {},
   "source": [
    "### 2 - Speech characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a993f-c72e-485b-a1f7-8d1fb6cea464",
   "metadata": {},
   "source": [
    "#### 2.1 - Single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf29a1d-d279-417c-b398-d0216bb757cf",
   "metadata": {},
   "source": [
    "Now we will continue with the speech characteristics function. First, we will need to transcribe our file, here we are using the 'vosk' transcription function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6792f67-c432-428c-8ff8-3c0f6a0693de",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_json, transcript_text = ow.speech_transcription_vosk(filepath = 'sample_data/audio_files/sentences_1_audio.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1635b-616e-4ca5-83f4-38dd94d3bc44",
   "metadata": {},
   "source": [
    "Then we will pass the JSON file from the transcription function directly to the speech characteristics function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd16110-1cc0-43f3-8eb6-d6b05cf314a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, turns, summary_sc = ow.speech_characteristics(json_conf = transcript_json, option = 'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8980efb-0208-40b4-9eae-01ffc1c6c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine summary data \n",
    "\n",
    "summary_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0418a-eb90-4ed4-a34d-ddbd573f0d3f",
   "metadata": {},
   "source": [
    "#### 2.2 - Multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb9d76-2c79-411a-ab87-4aaba263b2f5",
   "metadata": {},
   "source": [
    "The below code will run the speech characteristics function on multiple files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e2d72-48df-44b2-b9e6-416d8e492954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'sample_data/audio_files'\n",
    "\n",
    "word_data = pd.DataFrame()\n",
    "turns_data = pd.DataFrame()\n",
    "summary_sc_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "  if filename.endswith('.wav'):\n",
    "    audio_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Transcribe\n",
    "    transcript_json, transcript_text = ow.speech_transcription_vosk(filepath = audio_path)\n",
    "    words, turns, summary_sc = ow.speech_characteristics(json_conf = transcript_json, option = 'simple')\n",
    "\n",
    "    # Here, make sure we can identify each file by adding the name in the first column of the dataframe, remove '.wav' from the name\n",
    "    filename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Add filename column as the first column using insert()\n",
    "    words.insert(0, 'filename', filename_no_ext)\n",
    "    turns.insert(0, 'filename', filename_no_ext)\n",
    "    summary_sc.insert(0, 'filename', filename_no_ext)\n",
    "\n",
    "    # Store results for each file in each dataframe\n",
    "    word_data = pd.concat([word_data, words], ignore_index=True)\n",
    "    turns_data = pd.concat([turns_data, turns], ignore_index=True)\n",
    "    summary_sc_data = pd.concat([summary_sc_data, summary_sc], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829aa5a-ee21-4a12-8cb0-6ba34931e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc82266-a9e4-4124-821c-9913da77ba7c",
   "metadata": {},
   "source": [
    "### 3 - Facial expressivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8a760-558c-452e-838d-e0e709e3e058",
   "metadata": {},
   "source": [
    "#### 3.1 - Single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e58aa-059b-441b-af68-0148d148caef",
   "metadata": {},
   "source": [
    "From here, let's take a look at the video data. We'll start with just running facial expressivity on a single video file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d8b5b-3961-475b-9689-ff24338940c5",
   "metadata": {},
   "source": [
    "The video used in this example is 18 seconds and estimated runtime of `facial_expressivity` is approximately the same. For longer videos, you should expect runtimes approximately proportionate to the video duration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee124e43-9574-48a7-894b-accdd081de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise_loc, framewise_disp, summary_fe = ow.facial_expressivity(filepath = 'sample_data/video_files/sentences_1_video.mp4', baseline_filepath = 'sample_data/video_files/baseline_videos/sentences_1_bl_video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342489b9-28da-4de9-9c6c-73fe42856dba",
   "metadata": {},
   "source": [
    "We can look at the output in a couple of ways. We can look at the `framewise_disp` output to get a sense of displacement for each facial landmark at each frame. This dataframe contains quite a bit of data, so we can also look at the `summary_fe` output which will give us an overall displacement summary for each composite facial area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c2c3a6-fbee-4bb6-ae5b-6c2b5d3c7897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "framewise_disp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62554ee9-e8bc-4c38-9b3f-0881e5d2f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9ca21-8d2d-4b7c-88f8-671e755efdf4",
   "metadata": {},
   "source": [
    "Just for demonstration, if we don't include a baseline video, the displacement calculations will differ: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a57689-4a8c-4864-931a-5567426e3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise_loc_nobl, framewise_disp_nobl, summary_fe_nobl = ow.facial_expressivity(filepath = 'sample_data/video_files/sentences_1_video.mp4')\n",
    "\n",
    "summary_fe_nobl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d42239-02ad-41d2-bd37-6bd9b897c879",
   "metadata": {},
   "source": [
    "#### 3.2 - Multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af50809-2aab-4c52-b7f7-45fa78281437",
   "metadata": {},
   "source": [
    "When running this function on multiple video files, make sure to match the video file to the baseline file using a subject identifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f53118-7ef2-4334-a046-bcc52085455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'sample_data/video_files'\n",
    "baseline_folder = 'sample_data/video_files/baseline_videos/'\n",
    "\n",
    "frames_data = pd.DataFrame()\n",
    "displacement_data = pd.DataFrame()\n",
    "summary_fe_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.mp4'):\n",
    "        video_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Extract identifier from filename (assuming a pattern like \"person1_video.mp4\")\n",
    "        identifier = \"_\".join(filename.split(\"_\")[:2])  \n",
    "        baseline_filename = f\"{identifier}_bl_video.mp4\"  # Construct baseline filename\n",
    "        baseline_filepath = os.path.join(baseline_folder, baseline_filename)\n",
    "        \n",
    "        # Run facial expressivity - this sample uses the same video as a baseline because the samples are of the same person\n",
    "        framewise_loc, framewise_disp, summary_fe = ow.facial_expressivity(filepath = video_path, baseline_filepath = baseline_filepath)\n",
    "    \n",
    "        # Here, make sure we can identify each file by adding the name in the first column of the dataframe, remove '.mp4' from the name\n",
    "        filename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Add filename column as the first column using insert()\n",
    "        framewise_loc.insert(0, 'filename', filename_no_ext)\n",
    "        framewise_disp.insert(0, 'filename', filename_no_ext)\n",
    "        summary_fe.insert(0, 'filename', filename_no_ext)\n",
    "\n",
    "        # Store results for each file in each dataframe\n",
    "        frames_data = pd.concat([frames_data, framewise_loc], ignore_index=True)\n",
    "        displacement_data = pd.concat([displacement_data, framewise_disp], ignore_index=True)\n",
    "        summary_fe_data = pd.concat([summary_fe_data, summary_fe], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe06a14-2b0b-4345-a080-f908309a66c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_fe_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9456b1b-1f95-4c41-8087-cca27f643587",
   "metadata": {},
   "source": [
    "### 4 - Emotional expressivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1f0b3-54a2-4968-a975-f5ac9014d25e",
   "metadata": {},
   "source": [
    "#### 4.1 - Single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d0d46-6d33-41d3-b45f-aad5a0670008",
   "metadata": {},
   "source": [
    "When running the `emotional_expressivity` function, be aware that the runtime is considerably slower than for `facial_expressivity`. For the 18 second video, processing time is approximately 50 seconds. For longer videos, plan for a processing time of about 2.5x the file length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea8330-11c0-444e-8313-018090efe79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise_ee, summary_ee = ow.emotional_expressivity(filepath = 'sample_data/video_files/sentences_1_video.mp4', baseline_filepath = 'sample_data/video_files/baseline_videos/sentences_1_bl_video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afedb57-98a8-41a2-a947-47766272b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebba60a-f120-45e6-8e54-f4affa0e9f5f",
   "metadata": {},
   "source": [
    "Again for demonstration without a baseline video, the expressivity metrics will differ: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb359a3-dbdd-4a98-8590-b12ef124073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise_ee_nobl, summary_ee_nobl = ow.emotional_expressivity(filepath = 'sample_data/video_files/sentences_1_video.mp4')\n",
    "\n",
    "summary_ee_nobl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aade75-3f34-4402-bb63-916213cd7f8e",
   "metadata": {},
   "source": [
    "#### 4.2 - Multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0608f79-88eb-4456-b5f3-77438b6075d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'sample_data/video_files'\n",
    "baseline_folder = 'sample_data/video_files/baseline_videos/'  \n",
    "\n",
    "frames_ee_data = pd.DataFrame()\n",
    "summary_ee_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.mp4'):\n",
    "        video_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Extract identifier from filename (assuming a pattern like \"person1_video.mp4\")\n",
    "        identifier = \"_\".join(filename.split(\"_\")[:2])  \n",
    "        baseline_filename = f\"{identifier}_bl_video.mp4\"  # Construct baseline filename\n",
    "        baseline_filepath = os.path.join(baseline_folder, baseline_filename)\n",
    "\n",
    "        # Run emotional expressivity - this sample uses a clip from the same video as a baseline because the samples are of the same person\n",
    "        framewise_ee, summary_ee = ow.emotional_expressivity(filepath = video_path, baseline_filepath = baseline_filepath)\n",
    "    \n",
    "        # Here, make sure we can identify each file by adding the name in the first column of the dataframe, remove '.mp4' from the name\n",
    "        filename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Add filename column as the first column using insert()\n",
    "        framewise_ee.insert(0, 'filename', filename_no_ext)\n",
    "        summary_ee.insert(0, 'filename', filename_no_ext)\n",
    "\n",
    "        # Store results for each file in each dataframe\n",
    "        frames_ee_data = pd.concat([frames_ee_data, framewise_ee], ignore_index=True)\n",
    "        summary_ee_data = pd.concat([summary_ee_data, summary_ee], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79458a56-e18f-4798-8399-fcd05bff7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ee_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
